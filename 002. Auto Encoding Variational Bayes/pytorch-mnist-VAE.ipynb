{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch VAE example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Basic VAE Example](https://github.com/pytorch/examples/blob/master/vae/main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code를 보기전에 VAE의 특징을 알아보자. \n",
    "\n",
    "- VAE는 Generative model이라는 것. (training data가 주어졌을 때 이 data가 sampling된 분포와 같은 분포에서 새로운 sample을 생성하는 model)\n",
    "- latent variable이라는 것이 있으며 이것을 바탕으로 데이터를 생성한다는 것(Decoder).\n",
    "- 문제를 더 쉽게 만들기 위해 latent variable 이라는 것을 Encoder를 통해 추출한다는 것.\n",
    "- VAE의 학습과정은 MLE라는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/ce7x00eq6eltvho/Screenshot%202018-06-19%2022.05.22.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p_\\theta(x)$ : probability density function ($\\theta$라는 parameter가 주어졌을때 $x$라는 data가 나올 확률)\n",
    "\n",
    "위 확률 밀도 함수를 **최대화**하는 것이 `Generative Model` 혹은 `density estimation`의 목표이다. \n",
    "\n",
    "$z$라는 **latent variable**을 사용해서 **식(1)**로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ p_\\theta(x) = \\int p_\\theta(z)p_\\theta(x|z)dz \\tag{1}\\label{eq1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**식(1)**을 그림으로 표현하면 다음과 같다. \n",
    "\n",
    "> <img src=\"https://www.dropbox.com/s/jyiu96dd3tp8rue/Screenshot%202018-06-20%2000.53.19.png?dl=1\" alt=\"drawing\" width=\"200\"/> - \n",
    "\n",
    "즉, `VAE`는 기존 `Auto-Encoder`와 달리 latent variable($z$)을 정의하는데, \n",
    "\n",
    "#### Q1) \n",
    "\n",
    "- latent variable($z$) 필요한 이유는 무엇일까? \n",
    "\n",
    "#### A1) \n",
    "\n",
    "- 일반적으로 생성하고 싶은 데이터들은 차원이 매우 높고 datapoint 사이에 복잡한 관계가 있다. 따라서, 이러한 관계를 확률 모델로 모델링하는게 아니라 데이터를 표현하는 $z$가 있으면 그 $z$로부터 데이터를 생성하는 graphical model을 생각해보는 것이다. (**manifold 가정**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로 이 식(1) 을 미분해서 그 미분값에 따라 **stochastic gradient ascent**를 할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식 (1)의 구성요소는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p_\\theta(z)$ : latent variable $z$를 sampling 할 수 있는 pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p_\\theta(x|z)$ : $z$가 주어졌을 때 $x$를 생성해내는 pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/h0kcdfe7r0o2lqa/Screenshot%202018-06-20%2008.09.47.png?dl=1\" alt=\"drawing\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "- stochastic gradient ascent를 하려면 미분을 해야하는데 왜 미분이 불가능할까?\n",
    "\n",
    "#### A2)\n",
    "- 오른쪽 항을 보면, 모든 $z$에 대해 integral(적분)을 해야하는데, 모든 $z$를 구할 수 없기 때문에 integral을 할 수 없으므로, 최적의 $\\theta$를 추정하기 위한 미분 역시 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼에도 불구하고, **MLE**문제를 풀기위해서는 미분을 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "- 어떻게 **MLE**를 풀 수 있을까? 우선, 미분을 위해 적분 문제를 해결할 수 있을까?\n",
    "\n",
    "#### A3) \n",
    "- integral의 경우 integral을 다 계산하지 않고 `Monte-Carlo estimation`을 통해 estimate 할 것입니다. 아래 식으로 표현함. 대부분의 $z$에 대해서는 simple한 Gaussian 분포로부터 sampling하기 때문에 $p_\\theta(x|z)$는 거의 0의 값을 가질 것이다. 따라서 sampling이 상당히 많이 필요합니다. 데이터셋이 클 경우에 이것은 너무 cost가 큽니다. 좀 더 efficient하게 이 sampling 과정을 진행하려면 data($x$)에 dependent하게 $z$를 sampling 할 필요가 있다. 여기서 **Bayesian이 등장**합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ p_\\theta(x) = \\int p_\\theta(z)p_\\theta(x|z)dz \\tag{1}\\label{eq1} \\approx \\frac{1}{N} \\sum_{i=1}^{N}p_\\theta(x|z^{i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 더 효율적으로 sampling하기 위해서 $x$에 dependent한 $z$를 sampling을 하는 $p_\\theta(z|x)$를 생각해보는 것이다. 이는 $x$가 주어졌을 때 $x$를 생성해낼 것 같은 $z$에 대한 확률분포를 만드는 것이다. 이를 bayes'rule을 이용하면 다음과 같이 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/fi8reyjhzjh2h46/Screenshot%202018-06-20%2008.49.00.png?dl=1\" alt=\"drawing\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그럼 이제, $p_\\theta(z|x)$를 미분하려고 위의 식 오른쪽 항을 미분하려는데, 우리가 알고 싶어(목적으로)하는 $p_\\theta(x)$를 모르기 때문에 미분이 불가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서 이 poterior를 approximate 하는 새로운 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $q_\\phi(z|x)$ : $\\phi$라는 새로운 parameter로 표현되는 함수. (encoder 역할, 원래 poseterior를 approximate 했기 때문에 error가 존재할 것이며, 이를 고려하여 lower bound를 정의한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lower bound를 고려하기 전에 VAE의 네트워크 구조를 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/dxn7qfpfztrjduh/Screenshot%202018-06-20%2008.53.53.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 1** : \"Encoder network\"는 $q_\\phi(z|x)$이며, $x$를 input으로 받아서 $z$ space 상에서 확률분포를 만든다. 이 확률 분포는 $Gaussian$으로 가정하자. 이렇게 $x$에 dependent한 $Gaussian$분포로부터 $z$를 sampling.\n",
    "\n",
    "**step 2** : \"Decoder network\"는 $p_\\theta(x|z)$이며, $x$의 space 상의 $Gaussian$ 또는 $Bernoulli$ 분포를 output으로 내놓는다. 그러면 $x$를 이 분포로부터 sampling할 수 있다.\n",
    "\n",
    "즉, 위와 같은 구조를 가지기 때문에 \"Auto-Encoder\"가 되는 것이며 학습이 되고 나면 latent variable $z$라는 data의 의미있는  representation을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ELBO(Evidence Lower Bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 VAE를 어떻게 학습시키는지를 살펴보기 위해 objective function을 변형시켜보겠습니다. log likelihood는 다음과 같습니다. 이 값을 최대화시키는 것이 목표입니다. 이 식 자체는 intractable 하기 때문에 변형이 필요합니다.\n",
    "\n",
    "> $$ \\log p_\\theta(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 `log likelihood`를 $q_\\phi(z|x)$로부터 sampling한 latent variable $z$에 대한 기댓값으로 변경하면 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\log p_\\theta(x^{(i)}) = \\mathbb{E}_{z~q_\\phi(z|x^{(i)})} [\\log p_\\theta(x^{(i)})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식에 Bayes' Rule을 적용해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$p_\\theta(z|x^{(i)}) = \\frac{p_\\theta(x^{(i)}|z)p_\\theta(z)}{p_\\theta (x^{(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_\\theta (x^{(i)})$를 기준으로 정리해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$p_\\theta (x^{(i)})= \\frac{p_\\theta(x^{(i)}|z)p_\\theta(z)}{p_\\theta(z|x^{(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 `log likelihood`에 대입해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\log p_\\theta(x^{(i)}) = \\mathbb{E}_{z~q_\\phi(z|x^{(i)})} [\\log \\frac{p_\\theta(x^{(i)}|z)p_\\theta(z)}{p_\\theta(z|x^{(i)})}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음에 expectation 안의 항에 같은 $q_\\phi(z|x^{i})$를 값을 곱하고 나눕니다.\n",
    "\n",
    "> $$ \\log p_\\theta(x^{(i)}) = \\mathbb{E}_{z~q_\\phi(z|x^{(i)})} [\\log \\frac{p_\\theta(x^{(i)}|z)p_\\theta(z)}{p_\\theta(z|x^{(i)})} \\times \\frac{q_\\phi(z|x^{(i)})}{q_\\phi(z|x^{(i)})}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때, $p_\\theta(z)$와 $q_\\phi(z|x^{(i)})$를 하나로 묶고 $p_\\theta(x^{(i)}|z)$와 $q_\\phi(z|x^{(i)})$를 하나로 묶어서 별도의 Expectation으로 내보내보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\log p_\\theta(x^{(i)}) = \\mathbb{E}_z[p_\\theta(x^{(i)}|z)]\\ - \\ \\mathbb{E}_z[\\log \\frac{q_\\phi(z|x^{(i)})}{p_\\theta(z)}] \\ + \\ \\mathbb{E}_z [\\log \\frac{q_\\phi(z|x^{(i)}}{p_\\theta(z|x^{(i)})}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우변의 두번째 항과 세번째 항은 잘 보면 **KL-Divergence**의 형태인 것을 알 수 있습니다. 따라서 KL의 형태로 바꿔쓰면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\log p_\\theta(x^{(i)}) = \\mathbb{E}_z[p_\\theta(x^{(i)}|z)]\\ - \\ D_{KL}[q_\\phi(z|x^{(i)})||p_\\theta(z)] \\ + \\ D_{KL} [q_\\phi(z|x^{(i)}||p_\\theta(z|x^{(i)})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우변의 첫번째 항 : $\\mathbb{E}_z[p_\\theta(x^{(i)}|z)]$ 의미 \n",
    "    - $q_\\phi(z|x^{(i)})$로부터 sampling한 $z$가 있으며, 그 $z$를 가지고 $p_\\theta(x^{(i)}|z)$가 $x^{(i)}$를 생성할 `log likelihood` 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우변의 두번째 항 : $D_{KL}[\\log \\frac{q_\\phi(z|x^{(i)})}{p_\\theta(z)}]$ 의미 \n",
    "    - prior인 $p_\\theta(z)$와 근사된 posterior인 $q_\\phi(z|x^{(i)})$ 사이의 KL-divergence\n",
    "    - 즉, 근사된 posterior의 분포가 얼마나 normal distribution과 가까운지에 대한 척도!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우변의 세번째 항 : $\\mathbb{E}_z [\\log \\frac{q_\\phi(z|x^{(i)}}{p_\\theta(z|x^{(i)})}]$ 의미 \n",
    "    - 원래의 posterior과 근사된 posterior의 차이로서 `approximation error`로 볼 수 있다.\n",
    "    - 하지만, 앞에서 살펴봤듯이 $p_\\theta(z|x^{(i)})$는 intractable 해서 세번째 항을 계산하기 어렵다. 하지만, KL 성질대로 세번째 항은 무조건 0보다 크거나 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/ph4mzl3un2ai0dx/Screenshot%202018-06-20%2013.47.01.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 첫번째 항과 두번째 항을 하나로 묶어주면 원래의 objective function에 대한 tractable한 lower bound를 정의할 수 있습니다. MLE 문제를 풀기 위해 objective function을 미분해서 gradient ascent 할 것입니다. Lower bound가 정의된다면 이 lower bound를 최대화하는 문제로 바꿀 수 있고 결국 lower bound의 gradient를 구하게 될 것입니다. lower bound의 두 항은 모두 미분가능하기 때문에(어떻게 미분가능한건지는 뒤에서 살펴보겠습니다) 이제 우리는 최적화를 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/6wm7uf3nejsp21t/Screenshot%202018-06-20%2013.57.12.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lower bound를 다시 정의하자면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$L(x^{(i)}, \\theta, \\phi) = \\mathbb{E}_z[p_\\theta(x^{(i)}|z)]\\ - \\ D_{KL}[q_\\phi(z|x^{(i)})||p_\\theta(z)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 lower bound 식은 evidence의 $\\log$ 값인 $p_\\theta(x^{(i)}$의 lower bound이기 때문에 **Evidence Lower Bound, ELBO**라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\log p_\\theta(x^{(i)}) \\ge L(x^{(i)}, \\theta, \\phi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 원래 $p_\\theta(x^{(i)})$를 최대화하는 문제는 다음과 같이 바뀝니다. (sampling 과정을 통해 구해지만 $x$는 모두 i.i.d로 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\theta^{*}, \\phi^{*} = argmax_{\\theta, \\phi} \\sum_{i=1}^{N} L(x^{(i)}, \\theta, \\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 **`ELBO`**를 전개하는 과정을 정리하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/2ro3zzf3dgo2v31/Screenshot%202018-06-20%2014.09.01.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 ELBO를 구하는 과정은 다음 그림을 통해 이해해볼 수 있습니다. x를 encoder의 input으로 집어넣으면 encoder는 latent space 상에서의 mean과 variance를 내보냅니다(이 때, mean과 variance는 latent vector의 dimension마다 하나씩입니다). 그러면 이 mean과 variance가 posterior를 나타내게 되고 prior와의 KL을 구할 수 있습니다. 그 이후에 $z$로부터 decoder는 data의 space 상의 mean과 variance를 내보냅니다(만약 decoder의 output을 gaussian이라고 가정했다면. Bernoulli 분포라고 가정했다면 다른 형태). 그러면 ELBO의 첫번째 항 값을 구할 수 있고 ELBO가 구해집니다. 구한 값에 Backprop을 해서 업데이트하면 VAE의 학습과정이 완성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/geesw2b5yt21bx7/Screenshot%202018-06-20%2014.10.27.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variational Inference & Reparameterization Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE가 하고 싶은 것은 명확합니다. 또한 그것을 가로막는 문제도 명확히 제시합니다.\n",
    "\n",
    "**목표**: efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables  \n",
    "**문제**: intractable posterior, large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$L(x^{(i)}, \\theta, \\phi) = \\mathbb{E}_z[p_\\theta(x^{(i)}|z)]\\ - \\ D_{KL}[q_\\phi(z|x^{(i)})||p_\\theta(z)] $$\n",
    "\n",
    "> $$ \\theta^{*}, \\phi^{*} = argmax_{\\theta, \\phi} \\sum_{i=1}^{N} L(x^{(i)}, \\theta, \\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식을 만족하는 parameter를 구하는 방법은 다음과 같다.\n",
    "\n",
    "방법 1 : **analytic** (Mean-Field Variational Bayes)  \n",
    "방법 2 : **stochastic gradient ascent**\n",
    "\n",
    "논문에서는 방법 1을 통해 하는 경우, likelihood function인 $p_\\theta(x|z)$가 NN과 같은 복잡한 함수로 표현될 경우 intractable하다고 함.  \n",
    "따라서 방법 2인, gradient를 구해서 stochastic하게 parameter를 업데이트하는 방식을 사용!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4)\n",
    "\n",
    "- $\\theta$에 대한 미분은 문제 없으나, $\\phi$에 대해서 미분하는 것은 불가능한것은 아니지만 왜 문제가 있다고 하는걸까? 이를 해결하기 위한 방법은? \n",
    "\n",
    "#### A4)\n",
    "\n",
    "- 첫번째 항이 문제가 있다. 예를 들어 기댓값 안에 있는 함수(pdf)를 $f(z)$로 가정해보자. 이 기댓값에 대한 미분은 다음과 같이 표현할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\nabla_{\\phi} \\mathbb{E}_{q_\\phi(z)}[f(z))] = \\int \\nabla_{\\phi}q_\\phi(z)f(z)dz $$\n",
    "\n",
    "> $$ = \\int q_\\phi(z) \\frac{\\nabla_{\\phi}q_\\phi(z)}{q_\\phi(z)}f(z)dz $$\n",
    "\n",
    "> $$ = \\mathbb{E}_{q_\\phi(z)}[f(z) \\nabla_{\\phi} \\log q_\\phi(z))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 위에서 구한 미분값은 monte-carlo estimation을 통해 estimate 할 수 있다. 이때, $z^{(i)}$는 $q_\\phi(z|x^{i})$로부터 sampling 한다.\n",
    "따라서 gradient 값은 sampling 때문에 variance가 높을 것이다. 이 경우에 의해 현실적으로 미분이 어렵다는 이유라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\frac{1}{L} \\sum_{i=1}^{L} f(z^{l}) \\nabla_{\\phi} \\log \\ q_\\phi(z^{(l)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variance가 크다는 문제점을 해결하기 위해 **VAE**는 **reparameterization trick**이라는 **technique**을 사용. 즉 $z$를 posterior $q_\\phi(z|x)$로부터 sampling 하는게 아니라 미분 가능한 함수 $q_\\phi(\\epsilon, x)$로부터 **deterministic**하게 정해진다고 보는 것이다. 이때 $\\epsilon$은 noise variable이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\tilde z = g_\\phi(\\epsilon, x) \\qquad where \\quad \\epsilon \\sim p(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우 다음과 같이 $p(z)$의 $q_\\phi(z)$에 대한 기댓값을 $\\epsilon$에 대한 기댓값으로 바꿀 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$ \\mathbb{E}_{q_\\phi(z|x^{(i)})}[f(z)] = \\mathbb{E}_{\\epsilon}[f(g_\\phi(\\epsilon, x^{(i)}))] = \\frac{1}{L}\\sum_{l=1}^{L}f(g_\\phi(z^{(l)}, x^{(i)})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 수식을 이용해서 **ELBO**를 고쳐쓸 수 있다. `SGVB(Stochastic Gradient Variational Bayes) estimator`라고 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $z^{(i, l)}$ = $g_\\phi(\\epsilon^{l}, x^{(i)})$, (여기서는  $g_\\phi(\\epsilon, x) = \\mu + \\sigma\\epsilon$으로 사용함(univariate gaussian case),\n",
    "- $\\epsilon^{(l)} \\sim p(\\theta)$ (헷갈림 $p(\\theta)$ 맞는지 확인 필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\tilde L ^{\\mathcal{B}} (x^{(i)}, \\theta, \\phi) = \\frac{1}{L}\\sum_{l=1}^{L}f(g_\\phi(x^{(i)}, z^{(i, l)}))- \\ D_{KL}[q_\\phi(z|x^{(i)})||p_\\theta(z)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 `reparameterization trick`을 그림으로 보자면 다음과 같습니다. 원래는 encoder로부터 구한 data dependent한 mean과 variance를 가지고 posterior를 만듭니다. 그 posterior로부터 $z$를 샘플링한 다음에 그 $z$를 가지고 decoder는 data를 generation 했습니다. 하지만 reparametization을 하면 computation graph 내의 sampling 과정이 noise sampling이 되어 옆으로 빠져버립니다. 따라서 Back propagation을 통해 decoder output으로부터 encoder까지 gradient가 전달될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/5249ixq6r4t38l8/Screenshot%202018-06-21%2000.57.08.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 업데이트를 하는 알고리즘이 Auto-Encoding Variational Bayes이며 다음과 같습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/hxacd2bhz1hi3yl/Screenshot%202018-06-21%2001.05.46.png?dl=1\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code에서 목적 함수가 가장 중요**\n",
    "\n",
    "목적 함수를 설정하기 전에 우선 가정이 필요하다. \n",
    "\n",
    "- prior와 posterior를 모두 gaussian으로 가정\n",
    "- likelihood를 Bernoulli라고 가정 \n",
    "\n",
    "**`ELBO`** 식은 다음과 같이 쓸 수 있습니다. \n",
    "\n",
    "- 참고 : https://docs.google.com/presentation/d/175UzsMfZQ8-uuTxGO8L05KjV3iz7LOCRSAwcVVhr3-s/edit#slide=id.p36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\tilde L ^{\\mathcal{B}} (x^{(i)}, \\theta, \\phi) = \\frac{1}{L}\\sum_{l=1}^{L}f(g_\\phi(x^{(i)}, z^{(i, l)}))- \\ D_{KL}[q_\\phi(z|x^{(i)})||p_\\theta(z)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오른쪽 변의 두번째 항은 다음과 같이 정리할 수 있다.\n",
    "- [`Multivariate normal distributions`인 경우의 $D_{KL}$ 전개 (Example 참고)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$D_{KL}[q_\\phi(z|x^{(i)})||p_\\theta(z)] = \\frac{1}{2} \\sum_{j=1}^{J}(1+\\log((\\alpha_j^{(i)})^2) - (\\mu_j^{(i)})^2 - (\\alpha_j^{(i)})^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오른쪽 변의 첫번째 항은 다음과 같이 정리할 수 있다.\n",
    "- $y$는 $z$와 decoder를 통해 나온 값 \n",
    "- 첫번째 항은 잘 보면 cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\frac{1}{L}\\sum_{l=1}^{L}f(g_\\phi(x^{(i)}, z^{(i, l)})) = \\frac{1}{L}\\sum_{l=1}^{L}((x_i \\log y_{(i,l)} + (1-x_i)(1-y_{(i,l)}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최종적으로 다음과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$\\tilde L ^{\\mathcal{B}} (x^{(i)}, \\theta, \\phi) =  \\frac{1}{L}\\sum_{l=1}^{L}((x_i \\log y_{(i,l)} + (1-x_i)(1-y_{(i,l)})) + \\frac{1}{2} \\sum_{j=1}^{J}(1+\\log((\\alpha_j^{(i)})^2) - (\\mu_j^{(i)})^2 - (\\alpha_j^{(i)})^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  8\n",
      "Current cuda device  0\n",
      "GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "# 현재 Setup 되어있는 device 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7bed501e10>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar) # variance는 0보다 크거나 같아야함, 하지만, logvar는 음의 값이 나올 수 있기 때문에 이를 양수로 만들어주는 과정\n",
    "        eps = torch.randn_like(std) # noise 부분\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        \n",
    "         # likelhood를 bern 분포로 가정했기 때문에 0~1이 나올 수 있는 sigmoid 함수 사용.\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.to(device), 'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 551.418213\n",
      "Train Epoch: 1 [4096/60000 (7%)]\tLoss: 197.076965\n",
      "Train Epoch: 1 [8192/60000 (14%)]\tLoss: 175.674347\n",
      "Train Epoch: 1 [12288/60000 (20%)]\tLoss: 159.718384\n",
      "Train Epoch: 1 [16384/60000 (27%)]\tLoss: 157.448380\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 144.584595\n",
      "Train Epoch: 1 [24576/60000 (41%)]\tLoss: 138.753418\n",
      "Train Epoch: 1 [28672/60000 (48%)]\tLoss: 132.026917\n",
      "Train Epoch: 1 [32768/60000 (55%)]\tLoss: 127.238968\n",
      "Train Epoch: 1 [36864/60000 (61%)]\tLoss: 125.149162\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 124.913773\n",
      "Train Epoch: 1 [45056/60000 (75%)]\tLoss: 132.296387\n",
      "Train Epoch: 1 [49152/60000 (82%)]\tLoss: 121.053223\n",
      "Train Epoch: 1 [53248/60000 (89%)]\tLoss: 124.774239\n",
      "Train Epoch: 1 [57344/60000 (96%)]\tLoss: 126.253571\n",
      "====> Epoch: 1 Average loss: 146.9338\n",
      "====> Test set loss: 119.2208\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 116.469223\n",
      "Train Epoch: 2 [4096/60000 (7%)]\tLoss: 121.991882\n",
      "Train Epoch: 2 [8192/60000 (14%)]\tLoss: 121.196198\n",
      "Train Epoch: 2 [12288/60000 (20%)]\tLoss: 125.618729\n",
      "Train Epoch: 2 [16384/60000 (27%)]\tLoss: 117.730705\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 113.690475\n",
      "Train Epoch: 2 [24576/60000 (41%)]\tLoss: 117.387291\n",
      "Train Epoch: 2 [28672/60000 (48%)]\tLoss: 113.796829\n",
      "Train Epoch: 2 [32768/60000 (55%)]\tLoss: 117.880951\n",
      "Train Epoch: 2 [36864/60000 (61%)]\tLoss: 112.174438\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 110.046761\n",
      "Train Epoch: 2 [45056/60000 (75%)]\tLoss: 112.491081\n",
      "Train Epoch: 2 [49152/60000 (82%)]\tLoss: 111.341331\n",
      "Train Epoch: 2 [53248/60000 (89%)]\tLoss: 115.983559\n",
      "Train Epoch: 2 [57344/60000 (96%)]\tLoss: 113.975021\n",
      "====> Epoch: 2 Average loss: 115.8327\n",
      "====> Test set loss: 111.9809\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 109.754303\n",
      "Train Epoch: 3 [4096/60000 (7%)]\tLoss: 111.301682\n",
      "Train Epoch: 3 [8192/60000 (14%)]\tLoss: 109.101463\n",
      "Train Epoch: 3 [12288/60000 (20%)]\tLoss: 111.302635\n",
      "Train Epoch: 3 [16384/60000 (27%)]\tLoss: 113.848602\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 110.248398\n",
      "Train Epoch: 3 [24576/60000 (41%)]\tLoss: 112.911270\n",
      "Train Epoch: 3 [28672/60000 (48%)]\tLoss: 114.242165\n",
      "Train Epoch: 3 [32768/60000 (55%)]\tLoss: 113.577843\n",
      "Train Epoch: 3 [36864/60000 (61%)]\tLoss: 109.814667\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 106.681244\n",
      "Train Epoch: 3 [45056/60000 (75%)]\tLoss: 114.559692\n",
      "Train Epoch: 3 [49152/60000 (82%)]\tLoss: 106.557709\n",
      "Train Epoch: 3 [53248/60000 (89%)]\tLoss: 108.329498\n",
      "Train Epoch: 3 [57344/60000 (96%)]\tLoss: 113.695740\n",
      "====> Epoch: 3 Average loss: 111.3244\n",
      "====> Test set loss: 109.2093\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 109.880798\n",
      "Train Epoch: 4 [4096/60000 (7%)]\tLoss: 108.796227\n",
      "Train Epoch: 4 [8192/60000 (14%)]\tLoss: 113.524391\n",
      "Train Epoch: 4 [12288/60000 (20%)]\tLoss: 111.054649\n",
      "Train Epoch: 4 [16384/60000 (27%)]\tLoss: 108.538422\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 116.118393\n",
      "Train Epoch: 4 [24576/60000 (41%)]\tLoss: 106.492828\n",
      "Train Epoch: 4 [28672/60000 (48%)]\tLoss: 106.269859\n",
      "Train Epoch: 4 [32768/60000 (55%)]\tLoss: 108.965759\n",
      "Train Epoch: 4 [36864/60000 (61%)]\tLoss: 111.283195\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 114.180634\n",
      "Train Epoch: 4 [45056/60000 (75%)]\tLoss: 107.645584\n",
      "Train Epoch: 4 [49152/60000 (82%)]\tLoss: 112.501541\n",
      "Train Epoch: 4 [53248/60000 (89%)]\tLoss: 106.129150\n",
      "Train Epoch: 4 [57344/60000 (96%)]\tLoss: 111.201958\n",
      "====> Epoch: 4 Average loss: 109.2723\n",
      "====> Test set loss: 108.1597\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 106.256851\n",
      "Train Epoch: 5 [4096/60000 (7%)]\tLoss: 113.135414\n",
      "Train Epoch: 5 [8192/60000 (14%)]\tLoss: 109.269295\n",
      "Train Epoch: 5 [12288/60000 (20%)]\tLoss: 111.057472\n",
      "Train Epoch: 5 [16384/60000 (27%)]\tLoss: 105.628525\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 100.205917\n",
      "Train Epoch: 5 [24576/60000 (41%)]\tLoss: 106.593002\n",
      "Train Epoch: 5 [28672/60000 (48%)]\tLoss: 108.686798\n",
      "Train Epoch: 5 [32768/60000 (55%)]\tLoss: 109.009583\n",
      "Train Epoch: 5 [36864/60000 (61%)]\tLoss: 108.150673\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 99.600655\n",
      "Train Epoch: 5 [45056/60000 (75%)]\tLoss: 111.290642\n",
      "Train Epoch: 5 [49152/60000 (82%)]\tLoss: 102.994400\n",
      "Train Epoch: 5 [53248/60000 (89%)]\tLoss: 104.687057\n",
      "Train Epoch: 5 [57344/60000 (96%)]\tLoss: 109.808266\n",
      "====> Epoch: 5 Average loss: 108.0395\n",
      "====> Test set loss: 107.0045\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 104.977341\n",
      "Train Epoch: 6 [4096/60000 (7%)]\tLoss: 109.497086\n",
      "Train Epoch: 6 [8192/60000 (14%)]\tLoss: 111.907524\n",
      "Train Epoch: 6 [12288/60000 (20%)]\tLoss: 105.619308\n",
      "Train Epoch: 6 [16384/60000 (27%)]\tLoss: 103.279808\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 108.915161\n",
      "Train Epoch: 6 [24576/60000 (41%)]\tLoss: 107.634773\n",
      "Train Epoch: 6 [28672/60000 (48%)]\tLoss: 110.449486\n",
      "Train Epoch: 6 [32768/60000 (55%)]\tLoss: 106.273483\n",
      "Train Epoch: 6 [36864/60000 (61%)]\tLoss: 106.391602\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 108.547867\n",
      "Train Epoch: 6 [45056/60000 (75%)]\tLoss: 110.843643\n",
      "Train Epoch: 6 [49152/60000 (82%)]\tLoss: 103.841980\n",
      "Train Epoch: 6 [53248/60000 (89%)]\tLoss: 109.728981\n",
      "Train Epoch: 6 [57344/60000 (96%)]\tLoss: 109.421562\n",
      "====> Epoch: 6 Average loss: 107.2543\n",
      "====> Test set loss: 106.5271\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 105.703888\n",
      "Train Epoch: 7 [4096/60000 (7%)]\tLoss: 111.072632\n",
      "Train Epoch: 7 [8192/60000 (14%)]\tLoss: 105.978859\n",
      "Train Epoch: 7 [12288/60000 (20%)]\tLoss: 105.682304\n",
      "Train Epoch: 7 [16384/60000 (27%)]\tLoss: 99.818306\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 107.772003\n",
      "Train Epoch: 7 [24576/60000 (41%)]\tLoss: 110.395782\n",
      "Train Epoch: 7 [28672/60000 (48%)]\tLoss: 112.982834\n",
      "Train Epoch: 7 [32768/60000 (55%)]\tLoss: 105.237160\n",
      "Train Epoch: 7 [36864/60000 (61%)]\tLoss: 109.053459\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 109.815567\n",
      "Train Epoch: 7 [45056/60000 (75%)]\tLoss: 108.064247\n",
      "Train Epoch: 7 [49152/60000 (82%)]\tLoss: 104.271103\n",
      "Train Epoch: 7 [53248/60000 (89%)]\tLoss: 108.470879\n",
      "Train Epoch: 7 [57344/60000 (96%)]\tLoss: 106.436111\n",
      "====> Epoch: 7 Average loss: 106.6123\n",
      "====> Test set loss: 106.0340\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 110.263336\n",
      "Train Epoch: 8 [4096/60000 (7%)]\tLoss: 105.111916\n",
      "Train Epoch: 8 [8192/60000 (14%)]\tLoss: 110.553459\n",
      "Train Epoch: 8 [12288/60000 (20%)]\tLoss: 107.056793\n",
      "Train Epoch: 8 [16384/60000 (27%)]\tLoss: 108.101311\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 107.493988\n",
      "Train Epoch: 8 [24576/60000 (41%)]\tLoss: 108.327415\n",
      "Train Epoch: 8 [28672/60000 (48%)]\tLoss: 102.522995\n",
      "Train Epoch: 8 [32768/60000 (55%)]\tLoss: 108.096695\n",
      "Train Epoch: 8 [36864/60000 (61%)]\tLoss: 108.475517\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 115.440506\n",
      "Train Epoch: 8 [45056/60000 (75%)]\tLoss: 106.801346\n",
      "Train Epoch: 8 [49152/60000 (82%)]\tLoss: 108.404160\n",
      "Train Epoch: 8 [53248/60000 (89%)]\tLoss: 105.440392\n",
      "Train Epoch: 8 [57344/60000 (96%)]\tLoss: 109.782944\n",
      "====> Epoch: 8 Average loss: 106.1722\n",
      "====> Test set loss: 105.4308\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 98.906639\n",
      "Train Epoch: 9 [4096/60000 (7%)]\tLoss: 101.654137\n",
      "Train Epoch: 9 [8192/60000 (14%)]\tLoss: 105.604019\n",
      "Train Epoch: 9 [12288/60000 (20%)]\tLoss: 104.568825\n",
      "Train Epoch: 9 [16384/60000 (27%)]\tLoss: 108.744179\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 109.300102\n",
      "Train Epoch: 9 [24576/60000 (41%)]\tLoss: 106.136414\n",
      "Train Epoch: 9 [28672/60000 (48%)]\tLoss: 108.840378\n",
      "Train Epoch: 9 [32768/60000 (55%)]\tLoss: 110.498787\n",
      "Train Epoch: 9 [36864/60000 (61%)]\tLoss: 114.437500\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 105.608299\n",
      "Train Epoch: 9 [45056/60000 (75%)]\tLoss: 105.516357\n",
      "Train Epoch: 9 [49152/60000 (82%)]\tLoss: 104.593155\n",
      "Train Epoch: 9 [53248/60000 (89%)]\tLoss: 101.838013\n",
      "Train Epoch: 9 [57344/60000 (96%)]\tLoss: 105.142044\n",
      "====> Epoch: 9 Average loss: 105.8061\n",
      "====> Test set loss: 105.6108\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 100.446701\n",
      "Train Epoch: 10 [4096/60000 (7%)]\tLoss: 102.271233\n",
      "Train Epoch: 10 [8192/60000 (14%)]\tLoss: 105.971069\n",
      "Train Epoch: 10 [12288/60000 (20%)]\tLoss: 105.085701\n",
      "Train Epoch: 10 [16384/60000 (27%)]\tLoss: 105.330818\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 105.158348\n",
      "Train Epoch: 10 [24576/60000 (41%)]\tLoss: 106.045868\n",
      "Train Epoch: 10 [28672/60000 (48%)]\tLoss: 101.062332\n",
      "Train Epoch: 10 [32768/60000 (55%)]\tLoss: 105.576035\n",
      "Train Epoch: 10 [36864/60000 (61%)]\tLoss: 104.439072\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 107.396385\n",
      "Train Epoch: 10 [45056/60000 (75%)]\tLoss: 108.969955\n",
      "Train Epoch: 10 [49152/60000 (82%)]\tLoss: 102.371536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [53248/60000 (89%)]\tLoss: 108.918694\n",
      "Train Epoch: 10 [57344/60000 (96%)]\tLoss: 106.574471\n",
      "====> Epoch: 10 Average loss: 105.4424\n",
      "====> Test set loss: 105.2621\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 108.370743\n",
      "Train Epoch: 11 [4096/60000 (7%)]\tLoss: 99.938118\n",
      "Train Epoch: 11 [8192/60000 (14%)]\tLoss: 100.691666\n",
      "Train Epoch: 11 [12288/60000 (20%)]\tLoss: 103.675430\n",
      "Train Epoch: 11 [16384/60000 (27%)]\tLoss: 105.675827\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 109.676590\n",
      "Train Epoch: 11 [24576/60000 (41%)]\tLoss: 106.190811\n",
      "Train Epoch: 11 [28672/60000 (48%)]\tLoss: 110.429276\n",
      "Train Epoch: 11 [32768/60000 (55%)]\tLoss: 101.939163\n",
      "Train Epoch: 11 [36864/60000 (61%)]\tLoss: 104.660645\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 104.573120\n",
      "Train Epoch: 11 [45056/60000 (75%)]\tLoss: 109.492279\n",
      "Train Epoch: 11 [49152/60000 (82%)]\tLoss: 104.001091\n",
      "Train Epoch: 11 [53248/60000 (89%)]\tLoss: 104.984619\n",
      "Train Epoch: 11 [57344/60000 (96%)]\tLoss: 102.436752\n",
      "====> Epoch: 11 Average loss: 105.2087\n",
      "====> Test set loss: 104.7285\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 101.228493\n",
      "Train Epoch: 12 [4096/60000 (7%)]\tLoss: 103.529488\n",
      "Train Epoch: 12 [8192/60000 (14%)]\tLoss: 106.587929\n",
      "Train Epoch: 12 [12288/60000 (20%)]\tLoss: 106.037888\n",
      "Train Epoch: 12 [16384/60000 (27%)]\tLoss: 101.741203\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 107.796799\n",
      "Train Epoch: 12 [24576/60000 (41%)]\tLoss: 108.877335\n",
      "Train Epoch: 12 [28672/60000 (48%)]\tLoss: 108.119064\n",
      "Train Epoch: 12 [32768/60000 (55%)]\tLoss: 102.982628\n",
      "Train Epoch: 12 [36864/60000 (61%)]\tLoss: 111.426086\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 102.519203\n",
      "Train Epoch: 12 [45056/60000 (75%)]\tLoss: 109.714485\n",
      "Train Epoch: 12 [49152/60000 (82%)]\tLoss: 104.397285\n",
      "Train Epoch: 12 [53248/60000 (89%)]\tLoss: 101.660934\n",
      "Train Epoch: 12 [57344/60000 (96%)]\tLoss: 103.599792\n",
      "====> Epoch: 12 Average loss: 104.9112\n",
      "====> Test set loss: 104.4340\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 108.429680\n",
      "Train Epoch: 13 [4096/60000 (7%)]\tLoss: 107.619492\n",
      "Train Epoch: 13 [8192/60000 (14%)]\tLoss: 106.285706\n",
      "Train Epoch: 13 [12288/60000 (20%)]\tLoss: 103.765434\n",
      "Train Epoch: 13 [16384/60000 (27%)]\tLoss: 107.187393\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 103.947586\n",
      "Train Epoch: 13 [24576/60000 (41%)]\tLoss: 104.111572\n",
      "Train Epoch: 13 [28672/60000 (48%)]\tLoss: 105.034004\n",
      "Train Epoch: 13 [32768/60000 (55%)]\tLoss: 107.622406\n",
      "Train Epoch: 13 [36864/60000 (61%)]\tLoss: 102.182961\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 104.995491\n",
      "Train Epoch: 13 [45056/60000 (75%)]\tLoss: 98.420166\n",
      "Train Epoch: 13 [49152/60000 (82%)]\tLoss: 100.767601\n",
      "Train Epoch: 13 [53248/60000 (89%)]\tLoss: 103.541580\n",
      "Train Epoch: 13 [57344/60000 (96%)]\tLoss: 110.445885\n",
      "====> Epoch: 13 Average loss: 104.6818\n",
      "====> Test set loss: 104.6050\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 102.402016\n",
      "Train Epoch: 14 [4096/60000 (7%)]\tLoss: 96.472046\n",
      "Train Epoch: 14 [8192/60000 (14%)]\tLoss: 104.998398\n",
      "Train Epoch: 14 [12288/60000 (20%)]\tLoss: 108.491814\n",
      "Train Epoch: 14 [16384/60000 (27%)]\tLoss: 109.622406\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 105.173798\n",
      "Train Epoch: 14 [24576/60000 (41%)]\tLoss: 102.014244\n",
      "Train Epoch: 14 [28672/60000 (48%)]\tLoss: 105.588432\n",
      "Train Epoch: 14 [32768/60000 (55%)]\tLoss: 106.848328\n",
      "Train Epoch: 14 [36864/60000 (61%)]\tLoss: 105.129585\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 105.778900\n",
      "Train Epoch: 14 [45056/60000 (75%)]\tLoss: 102.020462\n",
      "Train Epoch: 14 [49152/60000 (82%)]\tLoss: 99.138596\n",
      "Train Epoch: 14 [53248/60000 (89%)]\tLoss: 107.187805\n",
      "Train Epoch: 14 [57344/60000 (96%)]\tLoss: 106.436935\n",
      "====> Epoch: 14 Average loss: 104.5058\n",
      "====> Test set loss: 104.2896\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 104.745049\n",
      "Train Epoch: 15 [4096/60000 (7%)]\tLoss: 100.243095\n",
      "Train Epoch: 15 [8192/60000 (14%)]\tLoss: 100.973381\n",
      "Train Epoch: 15 [12288/60000 (20%)]\tLoss: 105.555695\n",
      "Train Epoch: 15 [16384/60000 (27%)]\tLoss: 97.585167\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 105.232651\n",
      "Train Epoch: 15 [24576/60000 (41%)]\tLoss: 103.204689\n",
      "Train Epoch: 15 [28672/60000 (48%)]\tLoss: 105.512802\n",
      "Train Epoch: 15 [32768/60000 (55%)]\tLoss: 104.587708\n",
      "Train Epoch: 15 [36864/60000 (61%)]\tLoss: 101.696693\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 102.867867\n",
      "Train Epoch: 15 [45056/60000 (75%)]\tLoss: 103.148888\n",
      "Train Epoch: 15 [49152/60000 (82%)]\tLoss: 106.172455\n",
      "Train Epoch: 15 [53248/60000 (89%)]\tLoss: 105.608994\n",
      "Train Epoch: 15 [57344/60000 (96%)]\tLoss: 109.861320\n",
      "====> Epoch: 15 Average loss: 104.3369\n",
      "====> Test set loss: 104.0356\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 101.862808\n",
      "Train Epoch: 16 [4096/60000 (7%)]\tLoss: 105.793594\n",
      "Train Epoch: 16 [8192/60000 (14%)]\tLoss: 97.115074\n",
      "Train Epoch: 16 [12288/60000 (20%)]\tLoss: 102.496330\n",
      "Train Epoch: 16 [16384/60000 (27%)]\tLoss: 104.089256\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 108.475899\n",
      "Train Epoch: 16 [24576/60000 (41%)]\tLoss: 103.426361\n",
      "Train Epoch: 16 [28672/60000 (48%)]\tLoss: 109.665756\n",
      "Train Epoch: 16 [32768/60000 (55%)]\tLoss: 108.439957\n",
      "Train Epoch: 16 [36864/60000 (61%)]\tLoss: 101.476547\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 99.182121\n",
      "Train Epoch: 16 [45056/60000 (75%)]\tLoss: 104.520142\n",
      "Train Epoch: 16 [49152/60000 (82%)]\tLoss: 100.578163\n",
      "Train Epoch: 16 [53248/60000 (89%)]\tLoss: 107.529228\n",
      "Train Epoch: 16 [57344/60000 (96%)]\tLoss: 104.099823\n",
      "====> Epoch: 16 Average loss: 104.1791\n",
      "====> Test set loss: 104.0711\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 101.397507\n",
      "Train Epoch: 17 [4096/60000 (7%)]\tLoss: 99.487213\n",
      "Train Epoch: 17 [8192/60000 (14%)]\tLoss: 102.359482\n",
      "Train Epoch: 17 [12288/60000 (20%)]\tLoss: 101.405373\n",
      "Train Epoch: 17 [16384/60000 (27%)]\tLoss: 106.122948\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 106.309738\n",
      "Train Epoch: 17 [24576/60000 (41%)]\tLoss: 106.295914\n",
      "Train Epoch: 17 [28672/60000 (48%)]\tLoss: 101.872795\n",
      "Train Epoch: 17 [32768/60000 (55%)]\tLoss: 103.071991\n",
      "Train Epoch: 17 [36864/60000 (61%)]\tLoss: 107.332870\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 107.216339\n",
      "Train Epoch: 17 [45056/60000 (75%)]\tLoss: 101.311768\n",
      "Train Epoch: 17 [49152/60000 (82%)]\tLoss: 109.773697\n",
      "Train Epoch: 17 [53248/60000 (89%)]\tLoss: 101.086006\n",
      "Train Epoch: 17 [57344/60000 (96%)]\tLoss: 106.418266\n",
      "====> Epoch: 17 Average loss: 103.9865\n",
      "====> Test set loss: 103.8629\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 104.455452\n",
      "Train Epoch: 18 [4096/60000 (7%)]\tLoss: 107.053223\n",
      "Train Epoch: 18 [8192/60000 (14%)]\tLoss: 101.197693\n",
      "Train Epoch: 18 [12288/60000 (20%)]\tLoss: 104.891129\n",
      "Train Epoch: 18 [16384/60000 (27%)]\tLoss: 110.808960\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 104.565300\n",
      "Train Epoch: 18 [24576/60000 (41%)]\tLoss: 96.153587\n",
      "Train Epoch: 18 [28672/60000 (48%)]\tLoss: 100.232422\n",
      "Train Epoch: 18 [32768/60000 (55%)]\tLoss: 102.128815\n",
      "Train Epoch: 18 [36864/60000 (61%)]\tLoss: 109.436844\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 103.534576\n",
      "Train Epoch: 18 [45056/60000 (75%)]\tLoss: 103.035950\n",
      "Train Epoch: 18 [49152/60000 (82%)]\tLoss: 103.584236\n",
      "Train Epoch: 18 [53248/60000 (89%)]\tLoss: 99.779770\n",
      "Train Epoch: 18 [57344/60000 (96%)]\tLoss: 101.635490\n",
      "====> Epoch: 18 Average loss: 103.8768\n",
      "====> Test set loss: 103.6807\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 107.287567\n",
      "Train Epoch: 19 [4096/60000 (7%)]\tLoss: 99.763367\n",
      "Train Epoch: 19 [8192/60000 (14%)]\tLoss: 101.512833\n",
      "Train Epoch: 19 [12288/60000 (20%)]\tLoss: 98.926453\n",
      "Train Epoch: 19 [16384/60000 (27%)]\tLoss: 97.893822\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 103.214409\n",
      "Train Epoch: 19 [24576/60000 (41%)]\tLoss: 96.551682\n",
      "Train Epoch: 19 [28672/60000 (48%)]\tLoss: 97.656563\n",
      "Train Epoch: 19 [32768/60000 (55%)]\tLoss: 105.287323\n",
      "Train Epoch: 19 [36864/60000 (61%)]\tLoss: 108.142059\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 105.258385\n",
      "Train Epoch: 19 [45056/60000 (75%)]\tLoss: 110.566940\n",
      "Train Epoch: 19 [49152/60000 (82%)]\tLoss: 103.320793\n",
      "Train Epoch: 19 [53248/60000 (89%)]\tLoss: 106.727478\n",
      "Train Epoch: 19 [57344/60000 (96%)]\tLoss: 100.715759\n",
      "====> Epoch: 19 Average loss: 103.7276\n",
      "====> Test set loss: 103.4739\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 109.827721\n",
      "Train Epoch: 20 [4096/60000 (7%)]\tLoss: 108.188591\n",
      "Train Epoch: 20 [8192/60000 (14%)]\tLoss: 105.636787\n",
      "Train Epoch: 20 [12288/60000 (20%)]\tLoss: 101.524612\n",
      "Train Epoch: 20 [16384/60000 (27%)]\tLoss: 107.453903\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 102.614700\n",
      "Train Epoch: 20 [24576/60000 (41%)]\tLoss: 110.119064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [28672/60000 (48%)]\tLoss: 108.264908\n",
      "Train Epoch: 20 [32768/60000 (55%)]\tLoss: 103.132866\n",
      "Train Epoch: 20 [36864/60000 (61%)]\tLoss: 104.293991\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 99.161003\n",
      "Train Epoch: 20 [45056/60000 (75%)]\tLoss: 104.364090\n",
      "Train Epoch: 20 [49152/60000 (82%)]\tLoss: 106.851753\n",
      "Train Epoch: 20 [53248/60000 (89%)]\tLoss: 101.759102\n",
      "Train Epoch: 20 [57344/60000 (96%)]\tLoss: 99.735367\n",
      "====> Epoch: 20 Average loss: 103.5720\n",
      "====> Test set loss: 103.6074\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 107.332169\n",
      "Train Epoch: 21 [4096/60000 (7%)]\tLoss: 107.285080\n",
      "Train Epoch: 21 [8192/60000 (14%)]\tLoss: 100.527344\n",
      "Train Epoch: 21 [12288/60000 (20%)]\tLoss: 102.157257\n",
      "Train Epoch: 21 [16384/60000 (27%)]\tLoss: 105.577484\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 101.020218\n",
      "Train Epoch: 21 [24576/60000 (41%)]\tLoss: 97.087471\n",
      "Train Epoch: 21 [28672/60000 (48%)]\tLoss: 101.934875\n",
      "Train Epoch: 21 [32768/60000 (55%)]\tLoss: 101.303162\n",
      "Train Epoch: 21 [36864/60000 (61%)]\tLoss: 102.403999\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 99.133400\n",
      "Train Epoch: 21 [45056/60000 (75%)]\tLoss: 100.474876\n",
      "Train Epoch: 21 [49152/60000 (82%)]\tLoss: 103.206566\n",
      "Train Epoch: 21 [53248/60000 (89%)]\tLoss: 102.900642\n",
      "Train Epoch: 21 [57344/60000 (96%)]\tLoss: 99.238266\n",
      "====> Epoch: 21 Average loss: 103.4173\n",
      "====> Test set loss: 103.5476\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 103.069855\n",
      "Train Epoch: 22 [4096/60000 (7%)]\tLoss: 107.646042\n",
      "Train Epoch: 22 [8192/60000 (14%)]\tLoss: 109.797905\n",
      "Train Epoch: 22 [12288/60000 (20%)]\tLoss: 102.155815\n",
      "Train Epoch: 22 [16384/60000 (27%)]\tLoss: 104.008232\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 103.162720\n",
      "Train Epoch: 22 [24576/60000 (41%)]\tLoss: 105.900162\n",
      "Train Epoch: 22 [28672/60000 (48%)]\tLoss: 107.144447\n",
      "Train Epoch: 22 [32768/60000 (55%)]\tLoss: 109.361771\n",
      "Train Epoch: 22 [36864/60000 (61%)]\tLoss: 106.845375\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 102.266235\n",
      "Train Epoch: 22 [45056/60000 (75%)]\tLoss: 102.163544\n",
      "Train Epoch: 22 [49152/60000 (82%)]\tLoss: 100.295273\n",
      "Train Epoch: 22 [53248/60000 (89%)]\tLoss: 102.688522\n",
      "Train Epoch: 22 [57344/60000 (96%)]\tLoss: 106.283836\n",
      "====> Epoch: 22 Average loss: 103.3144\n",
      "====> Test set loss: 103.0327\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 99.040016\n",
      "Train Epoch: 23 [4096/60000 (7%)]\tLoss: 101.624832\n",
      "Train Epoch: 23 [8192/60000 (14%)]\tLoss: 102.631821\n",
      "Train Epoch: 23 [12288/60000 (20%)]\tLoss: 108.593445\n",
      "Train Epoch: 23 [16384/60000 (27%)]\tLoss: 101.253746\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 103.959259\n",
      "Train Epoch: 23 [24576/60000 (41%)]\tLoss: 96.618919\n",
      "Train Epoch: 23 [28672/60000 (48%)]\tLoss: 103.930916\n",
      "Train Epoch: 23 [32768/60000 (55%)]\tLoss: 99.958862\n",
      "Train Epoch: 23 [36864/60000 (61%)]\tLoss: 105.873825\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 101.358200\n",
      "Train Epoch: 23 [45056/60000 (75%)]\tLoss: 103.990509\n",
      "Train Epoch: 23 [49152/60000 (82%)]\tLoss: 103.272598\n",
      "Train Epoch: 23 [53248/60000 (89%)]\tLoss: 100.578339\n",
      "Train Epoch: 23 [57344/60000 (96%)]\tLoss: 109.286018\n",
      "====> Epoch: 23 Average loss: 103.2593\n",
      "====> Test set loss: 103.1073\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 105.771400\n",
      "Train Epoch: 24 [4096/60000 (7%)]\tLoss: 100.784912\n",
      "Train Epoch: 24 [8192/60000 (14%)]\tLoss: 101.181976\n",
      "Train Epoch: 24 [12288/60000 (20%)]\tLoss: 104.640533\n",
      "Train Epoch: 24 [16384/60000 (27%)]\tLoss: 98.526894\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 101.878044\n",
      "Train Epoch: 24 [24576/60000 (41%)]\tLoss: 108.898071\n",
      "Train Epoch: 24 [28672/60000 (48%)]\tLoss: 99.953598\n",
      "Train Epoch: 24 [32768/60000 (55%)]\tLoss: 98.002220\n",
      "Train Epoch: 24 [36864/60000 (61%)]\tLoss: 102.684296\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 99.871140\n",
      "Train Epoch: 24 [45056/60000 (75%)]\tLoss: 102.790070\n",
      "Train Epoch: 24 [49152/60000 (82%)]\tLoss: 100.257767\n",
      "Train Epoch: 24 [53248/60000 (89%)]\tLoss: 106.147346\n",
      "Train Epoch: 24 [57344/60000 (96%)]\tLoss: 101.809906\n",
      "====> Epoch: 24 Average loss: 103.1718\n",
      "====> Test set loss: 103.2761\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 103.761032\n",
      "Train Epoch: 25 [4096/60000 (7%)]\tLoss: 103.941986\n",
      "Train Epoch: 25 [8192/60000 (14%)]\tLoss: 101.681519\n",
      "Train Epoch: 25 [12288/60000 (20%)]\tLoss: 100.813461\n",
      "Train Epoch: 25 [16384/60000 (27%)]\tLoss: 100.131638\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 105.013290\n",
      "Train Epoch: 25 [24576/60000 (41%)]\tLoss: 103.853439\n",
      "Train Epoch: 25 [28672/60000 (48%)]\tLoss: 100.077934\n",
      "Train Epoch: 25 [32768/60000 (55%)]\tLoss: 100.636284\n",
      "Train Epoch: 25 [36864/60000 (61%)]\tLoss: 101.821381\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 105.420197\n",
      "Train Epoch: 25 [45056/60000 (75%)]\tLoss: 102.333969\n",
      "Train Epoch: 25 [49152/60000 (82%)]\tLoss: 111.262962\n",
      "Train Epoch: 25 [53248/60000 (89%)]\tLoss: 98.649765\n",
      "Train Epoch: 25 [57344/60000 (96%)]\tLoss: 98.685249\n",
      "====> Epoch: 25 Average loss: 103.0430\n",
      "====> Test set loss: 103.2950\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 105.779037\n",
      "Train Epoch: 26 [4096/60000 (7%)]\tLoss: 107.361336\n",
      "Train Epoch: 26 [8192/60000 (14%)]\tLoss: 105.309311\n",
      "Train Epoch: 26 [12288/60000 (20%)]\tLoss: 102.496086\n",
      "Train Epoch: 26 [16384/60000 (27%)]\tLoss: 100.645401\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 96.006882\n",
      "Train Epoch: 26 [24576/60000 (41%)]\tLoss: 106.243820\n",
      "Train Epoch: 26 [28672/60000 (48%)]\tLoss: 104.246750\n",
      "Train Epoch: 26 [32768/60000 (55%)]\tLoss: 102.888023\n",
      "Train Epoch: 26 [36864/60000 (61%)]\tLoss: 104.008881\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 102.736206\n",
      "Train Epoch: 26 [45056/60000 (75%)]\tLoss: 105.473503\n",
      "Train Epoch: 26 [49152/60000 (82%)]\tLoss: 101.646378\n",
      "Train Epoch: 26 [53248/60000 (89%)]\tLoss: 102.215874\n",
      "Train Epoch: 26 [57344/60000 (96%)]\tLoss: 98.192146\n",
      "====> Epoch: 26 Average loss: 102.9398\n",
      "====> Test set loss: 102.8853\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 103.506004\n",
      "Train Epoch: 27 [4096/60000 (7%)]\tLoss: 103.083374\n",
      "Train Epoch: 27 [8192/60000 (14%)]\tLoss: 107.262520\n",
      "Train Epoch: 27 [12288/60000 (20%)]\tLoss: 105.313484\n",
      "Train Epoch: 27 [16384/60000 (27%)]\tLoss: 100.869492\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 100.475098\n",
      "Train Epoch: 27 [24576/60000 (41%)]\tLoss: 104.863625\n",
      "Train Epoch: 27 [28672/60000 (48%)]\tLoss: 104.333588\n",
      "Train Epoch: 27 [32768/60000 (55%)]\tLoss: 101.277496\n",
      "Train Epoch: 27 [36864/60000 (61%)]\tLoss: 105.479782\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 106.461754\n",
      "Train Epoch: 27 [45056/60000 (75%)]\tLoss: 104.864494\n",
      "Train Epoch: 27 [49152/60000 (82%)]\tLoss: 107.184288\n",
      "Train Epoch: 27 [53248/60000 (89%)]\tLoss: 99.622742\n",
      "Train Epoch: 27 [57344/60000 (96%)]\tLoss: 103.547974\n",
      "====> Epoch: 27 Average loss: 102.8479\n",
      "====> Test set loss: 102.7952\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 97.686569\n",
      "Train Epoch: 28 [4096/60000 (7%)]\tLoss: 98.928955\n",
      "Train Epoch: 28 [8192/60000 (14%)]\tLoss: 100.143356\n",
      "Train Epoch: 28 [12288/60000 (20%)]\tLoss: 99.787048\n",
      "Train Epoch: 28 [16384/60000 (27%)]\tLoss: 95.404022\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 102.615097\n",
      "Train Epoch: 28 [24576/60000 (41%)]\tLoss: 104.930420\n",
      "Train Epoch: 28 [28672/60000 (48%)]\tLoss: 99.935272\n",
      "Train Epoch: 28 [32768/60000 (55%)]\tLoss: 106.037079\n",
      "Train Epoch: 28 [36864/60000 (61%)]\tLoss: 103.276901\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 99.142929\n",
      "Train Epoch: 28 [45056/60000 (75%)]\tLoss: 100.864273\n",
      "Train Epoch: 28 [49152/60000 (82%)]\tLoss: 108.321899\n",
      "Train Epoch: 28 [53248/60000 (89%)]\tLoss: 102.429695\n",
      "Train Epoch: 28 [57344/60000 (96%)]\tLoss: 104.140381\n",
      "====> Epoch: 28 Average loss: 102.7398\n",
      "====> Test set loss: 102.9573\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 99.274521\n",
      "Train Epoch: 29 [4096/60000 (7%)]\tLoss: 104.202820\n",
      "Train Epoch: 29 [8192/60000 (14%)]\tLoss: 106.002457\n",
      "Train Epoch: 29 [12288/60000 (20%)]\tLoss: 106.020226\n",
      "Train Epoch: 29 [16384/60000 (27%)]\tLoss: 101.427444\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 105.129333\n",
      "Train Epoch: 29 [24576/60000 (41%)]\tLoss: 99.088318\n",
      "Train Epoch: 29 [28672/60000 (48%)]\tLoss: 98.187134\n",
      "Train Epoch: 29 [32768/60000 (55%)]\tLoss: 100.737839\n",
      "Train Epoch: 29 [36864/60000 (61%)]\tLoss: 107.647301\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 100.907974\n",
      "Train Epoch: 29 [45056/60000 (75%)]\tLoss: 100.005661\n",
      "Train Epoch: 29 [49152/60000 (82%)]\tLoss: 100.749817\n",
      "Train Epoch: 29 [53248/60000 (89%)]\tLoss: 100.290916\n",
      "Train Epoch: 29 [57344/60000 (96%)]\tLoss: 108.322357\n",
      "====> Epoch: 29 Average loss: 102.7406\n",
      "====> Test set loss: 102.7017\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 106.311386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [4096/60000 (7%)]\tLoss: 103.586006\n",
      "Train Epoch: 30 [8192/60000 (14%)]\tLoss: 101.982567\n",
      "Train Epoch: 30 [12288/60000 (20%)]\tLoss: 90.697235\n",
      "Train Epoch: 30 [16384/60000 (27%)]\tLoss: 98.922165\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 102.408646\n",
      "Train Epoch: 30 [24576/60000 (41%)]\tLoss: 97.862816\n",
      "Train Epoch: 30 [28672/60000 (48%)]\tLoss: 98.727127\n",
      "Train Epoch: 30 [32768/60000 (55%)]\tLoss: 102.017685\n",
      "Train Epoch: 30 [36864/60000 (61%)]\tLoss: 103.044113\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 99.018600\n",
      "Train Epoch: 30 [45056/60000 (75%)]\tLoss: 104.519981\n",
      "Train Epoch: 30 [49152/60000 (82%)]\tLoss: 100.125694\n",
      "Train Epoch: 30 [53248/60000 (89%)]\tLoss: 98.638557\n",
      "Train Epoch: 30 [57344/60000 (96%)]\tLoss: 97.372757\n",
      "====> Epoch: 30 Average loss: 102.6424\n",
      "====> Test set loss: 102.6840\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 105.473518\n",
      "Train Epoch: 31 [4096/60000 (7%)]\tLoss: 103.355019\n",
      "Train Epoch: 31 [8192/60000 (14%)]\tLoss: 106.950989\n",
      "Train Epoch: 31 [12288/60000 (20%)]\tLoss: 105.102135\n",
      "Train Epoch: 31 [16384/60000 (27%)]\tLoss: 104.774590\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 99.809570\n",
      "Train Epoch: 31 [24576/60000 (41%)]\tLoss: 101.176491\n",
      "Train Epoch: 31 [28672/60000 (48%)]\tLoss: 101.696991\n",
      "Train Epoch: 31 [32768/60000 (55%)]\tLoss: 111.161087\n",
      "Train Epoch: 31 [36864/60000 (61%)]\tLoss: 106.477646\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 99.398842\n",
      "Train Epoch: 31 [45056/60000 (75%)]\tLoss: 103.065147\n",
      "Train Epoch: 31 [49152/60000 (82%)]\tLoss: 99.265594\n",
      "Train Epoch: 31 [53248/60000 (89%)]\tLoss: 105.866829\n",
      "Train Epoch: 31 [57344/60000 (96%)]\tLoss: 106.532333\n",
      "====> Epoch: 31 Average loss: 102.5572\n",
      "====> Test set loss: 102.7589\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 100.833847\n",
      "Train Epoch: 32 [4096/60000 (7%)]\tLoss: 102.250824\n",
      "Train Epoch: 32 [8192/60000 (14%)]\tLoss: 103.473686\n",
      "Train Epoch: 32 [12288/60000 (20%)]\tLoss: 103.797195\n",
      "Train Epoch: 32 [16384/60000 (27%)]\tLoss: 104.514969\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 100.078651\n",
      "Train Epoch: 32 [24576/60000 (41%)]\tLoss: 103.327667\n",
      "Train Epoch: 32 [28672/60000 (48%)]\tLoss: 107.783752\n",
      "Train Epoch: 32 [32768/60000 (55%)]\tLoss: 106.251137\n",
      "Train Epoch: 32 [36864/60000 (61%)]\tLoss: 102.247704\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 102.252396\n",
      "Train Epoch: 32 [45056/60000 (75%)]\tLoss: 105.633034\n",
      "Train Epoch: 32 [49152/60000 (82%)]\tLoss: 102.494675\n",
      "Train Epoch: 32 [53248/60000 (89%)]\tLoss: 102.724663\n",
      "Train Epoch: 32 [57344/60000 (96%)]\tLoss: 105.603073\n",
      "====> Epoch: 32 Average loss: 102.4814\n",
      "====> Test set loss: 102.5394\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 100.853897\n",
      "Train Epoch: 33 [4096/60000 (7%)]\tLoss: 108.511200\n",
      "Train Epoch: 33 [8192/60000 (14%)]\tLoss: 101.257385\n",
      "Train Epoch: 33 [12288/60000 (20%)]\tLoss: 106.396774\n",
      "Train Epoch: 33 [16384/60000 (27%)]\tLoss: 104.709900\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 104.542084\n",
      "Train Epoch: 33 [24576/60000 (41%)]\tLoss: 95.202171\n",
      "Train Epoch: 33 [28672/60000 (48%)]\tLoss: 97.177658\n",
      "Train Epoch: 33 [32768/60000 (55%)]\tLoss: 102.784782\n",
      "Train Epoch: 33 [36864/60000 (61%)]\tLoss: 103.946121\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 105.149132\n",
      "Train Epoch: 33 [45056/60000 (75%)]\tLoss: 103.773560\n",
      "Train Epoch: 33 [49152/60000 (82%)]\tLoss: 100.860725\n",
      "Train Epoch: 33 [53248/60000 (89%)]\tLoss: 103.221924\n",
      "Train Epoch: 33 [57344/60000 (96%)]\tLoss: 100.659470\n",
      "====> Epoch: 33 Average loss: 102.4486\n",
      "====> Test set loss: 102.7883\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 102.904228\n",
      "Train Epoch: 34 [4096/60000 (7%)]\tLoss: 102.714867\n",
      "Train Epoch: 34 [8192/60000 (14%)]\tLoss: 101.969193\n",
      "Train Epoch: 34 [12288/60000 (20%)]\tLoss: 97.036423\n",
      "Train Epoch: 34 [16384/60000 (27%)]\tLoss: 98.926544\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 105.692734\n",
      "Train Epoch: 34 [24576/60000 (41%)]\tLoss: 103.002235\n",
      "Train Epoch: 34 [28672/60000 (48%)]\tLoss: 102.395103\n",
      "Train Epoch: 34 [32768/60000 (55%)]\tLoss: 98.500656\n",
      "Train Epoch: 34 [36864/60000 (61%)]\tLoss: 104.124100\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 100.526306\n",
      "Train Epoch: 34 [45056/60000 (75%)]\tLoss: 101.313148\n",
      "Train Epoch: 34 [49152/60000 (82%)]\tLoss: 105.568344\n",
      "Train Epoch: 34 [53248/60000 (89%)]\tLoss: 103.705750\n",
      "Train Epoch: 34 [57344/60000 (96%)]\tLoss: 98.592102\n",
      "====> Epoch: 34 Average loss: 102.3662\n",
      "====> Test set loss: 102.4434\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 102.588585\n",
      "Train Epoch: 35 [4096/60000 (7%)]\tLoss: 100.922729\n",
      "Train Epoch: 35 [8192/60000 (14%)]\tLoss: 103.328232\n",
      "Train Epoch: 35 [12288/60000 (20%)]\tLoss: 107.300575\n",
      "Train Epoch: 35 [16384/60000 (27%)]\tLoss: 101.100922\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 100.966034\n",
      "Train Epoch: 35 [24576/60000 (41%)]\tLoss: 102.292809\n",
      "Train Epoch: 35 [28672/60000 (48%)]\tLoss: 105.465416\n",
      "Train Epoch: 35 [32768/60000 (55%)]\tLoss: 103.447952\n",
      "Train Epoch: 35 [36864/60000 (61%)]\tLoss: 101.338295\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 105.081001\n",
      "Train Epoch: 35 [45056/60000 (75%)]\tLoss: 104.390991\n",
      "Train Epoch: 35 [49152/60000 (82%)]\tLoss: 99.531799\n",
      "Train Epoch: 35 [53248/60000 (89%)]\tLoss: 100.431671\n",
      "Train Epoch: 35 [57344/60000 (96%)]\tLoss: 100.202148\n",
      "====> Epoch: 35 Average loss: 102.2807\n",
      "====> Test set loss: 102.3093\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 100.413940\n",
      "Train Epoch: 36 [4096/60000 (7%)]\tLoss: 102.702103\n",
      "Train Epoch: 36 [8192/60000 (14%)]\tLoss: 96.031395\n",
      "Train Epoch: 36 [12288/60000 (20%)]\tLoss: 106.404762\n",
      "Train Epoch: 36 [16384/60000 (27%)]\tLoss: 98.933182\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 96.945343\n",
      "Train Epoch: 36 [24576/60000 (41%)]\tLoss: 103.875420\n",
      "Train Epoch: 36 [28672/60000 (48%)]\tLoss: 99.951385\n",
      "Train Epoch: 36 [32768/60000 (55%)]\tLoss: 99.318314\n",
      "Train Epoch: 36 [36864/60000 (61%)]\tLoss: 104.142441\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 101.535065\n",
      "Train Epoch: 36 [45056/60000 (75%)]\tLoss: 109.445068\n",
      "Train Epoch: 36 [49152/60000 (82%)]\tLoss: 108.705421\n",
      "Train Epoch: 36 [53248/60000 (89%)]\tLoss: 105.386154\n",
      "Train Epoch: 36 [57344/60000 (96%)]\tLoss: 104.184860\n",
      "====> Epoch: 36 Average loss: 102.2300\n",
      "====> Test set loss: 102.5109\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 108.285690\n",
      "Train Epoch: 37 [4096/60000 (7%)]\tLoss: 104.532410\n",
      "Train Epoch: 37 [8192/60000 (14%)]\tLoss: 101.919365\n",
      "Train Epoch: 37 [12288/60000 (20%)]\tLoss: 106.635498\n",
      "Train Epoch: 37 [16384/60000 (27%)]\tLoss: 102.948029\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 106.424240\n",
      "Train Epoch: 37 [24576/60000 (41%)]\tLoss: 104.928024\n",
      "Train Epoch: 37 [28672/60000 (48%)]\tLoss: 100.181366\n",
      "Train Epoch: 37 [32768/60000 (55%)]\tLoss: 100.971375\n",
      "Train Epoch: 37 [36864/60000 (61%)]\tLoss: 103.126442\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 103.598679\n",
      "Train Epoch: 37 [45056/60000 (75%)]\tLoss: 103.879654\n",
      "Train Epoch: 37 [49152/60000 (82%)]\tLoss: 105.713730\n",
      "Train Epoch: 37 [53248/60000 (89%)]\tLoss: 97.723534\n",
      "Train Epoch: 37 [57344/60000 (96%)]\tLoss: 101.778946\n",
      "====> Epoch: 37 Average loss: 102.1943\n",
      "====> Test set loss: 102.2917\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 102.736458\n",
      "Train Epoch: 38 [4096/60000 (7%)]\tLoss: 101.137917\n",
      "Train Epoch: 38 [8192/60000 (14%)]\tLoss: 109.036903\n",
      "Train Epoch: 38 [12288/60000 (20%)]\tLoss: 101.954597\n",
      "Train Epoch: 38 [16384/60000 (27%)]\tLoss: 105.585663\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 97.594139\n",
      "Train Epoch: 38 [24576/60000 (41%)]\tLoss: 104.048691\n",
      "Train Epoch: 38 [28672/60000 (48%)]\tLoss: 103.702637\n",
      "Train Epoch: 38 [32768/60000 (55%)]\tLoss: 98.811508\n",
      "Train Epoch: 38 [36864/60000 (61%)]\tLoss: 105.748398\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 100.671188\n",
      "Train Epoch: 38 [45056/60000 (75%)]\tLoss: 106.201385\n",
      "Train Epoch: 38 [49152/60000 (82%)]\tLoss: 101.099899\n",
      "Train Epoch: 38 [53248/60000 (89%)]\tLoss: 105.928055\n",
      "Train Epoch: 38 [57344/60000 (96%)]\tLoss: 106.400551\n",
      "====> Epoch: 38 Average loss: 102.0753\n",
      "====> Test set loss: 102.0267\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 103.380646\n",
      "Train Epoch: 39 [4096/60000 (7%)]\tLoss: 101.786659\n",
      "Train Epoch: 39 [8192/60000 (14%)]\tLoss: 101.406754\n",
      "Train Epoch: 39 [12288/60000 (20%)]\tLoss: 103.613235\n",
      "Train Epoch: 39 [16384/60000 (27%)]\tLoss: 102.349251\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 97.318748\n",
      "Train Epoch: 39 [24576/60000 (41%)]\tLoss: 104.390404\n",
      "Train Epoch: 39 [28672/60000 (48%)]\tLoss: 101.169075\n",
      "Train Epoch: 39 [32768/60000 (55%)]\tLoss: 101.803528\n",
      "Train Epoch: 39 [36864/60000 (61%)]\tLoss: 97.441025\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 102.691231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39 [45056/60000 (75%)]\tLoss: 103.838333\n",
      "Train Epoch: 39 [49152/60000 (82%)]\tLoss: 108.141548\n",
      "Train Epoch: 39 [53248/60000 (89%)]\tLoss: 103.583641\n",
      "Train Epoch: 39 [57344/60000 (96%)]\tLoss: 104.378235\n",
      "====> Epoch: 39 Average loss: 102.0648\n",
      "====> Test set loss: 102.2140\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 99.353729\n",
      "Train Epoch: 40 [4096/60000 (7%)]\tLoss: 101.897331\n",
      "Train Epoch: 40 [8192/60000 (14%)]\tLoss: 103.665031\n",
      "Train Epoch: 40 [12288/60000 (20%)]\tLoss: 103.305595\n",
      "Train Epoch: 40 [16384/60000 (27%)]\tLoss: 97.605614\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 106.488686\n",
      "Train Epoch: 40 [24576/60000 (41%)]\tLoss: 103.581192\n",
      "Train Epoch: 40 [28672/60000 (48%)]\tLoss: 102.906792\n",
      "Train Epoch: 40 [32768/60000 (55%)]\tLoss: 103.092415\n",
      "Train Epoch: 40 [36864/60000 (61%)]\tLoss: 101.921837\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 96.524292\n",
      "Train Epoch: 40 [45056/60000 (75%)]\tLoss: 105.983932\n",
      "Train Epoch: 40 [49152/60000 (82%)]\tLoss: 104.604836\n",
      "Train Epoch: 40 [53248/60000 (89%)]\tLoss: 99.907715\n",
      "Train Epoch: 40 [57344/60000 (96%)]\tLoss: 98.517181\n",
      "====> Epoch: 40 Average loss: 102.0045\n",
      "====> Test set loss: 102.2273\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 105.571838\n",
      "Train Epoch: 41 [4096/60000 (7%)]\tLoss: 100.452850\n",
      "Train Epoch: 41 [8192/60000 (14%)]\tLoss: 99.814835\n",
      "Train Epoch: 41 [12288/60000 (20%)]\tLoss: 107.038910\n",
      "Train Epoch: 41 [16384/60000 (27%)]\tLoss: 102.478394\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 100.529251\n",
      "Train Epoch: 41 [24576/60000 (41%)]\tLoss: 106.650398\n",
      "Train Epoch: 41 [28672/60000 (48%)]\tLoss: 97.141182\n",
      "Train Epoch: 41 [32768/60000 (55%)]\tLoss: 93.968346\n",
      "Train Epoch: 41 [36864/60000 (61%)]\tLoss: 100.377548\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 100.584351\n",
      "Train Epoch: 41 [45056/60000 (75%)]\tLoss: 102.027420\n",
      "Train Epoch: 41 [49152/60000 (82%)]\tLoss: 105.673767\n",
      "Train Epoch: 41 [53248/60000 (89%)]\tLoss: 104.151047\n",
      "Train Epoch: 41 [57344/60000 (96%)]\tLoss: 100.398468\n",
      "====> Epoch: 41 Average loss: 101.9329\n",
      "====> Test set loss: 102.2622\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 107.345406\n",
      "Train Epoch: 42 [4096/60000 (7%)]\tLoss: 101.790390\n",
      "Train Epoch: 42 [8192/60000 (14%)]\tLoss: 100.504601\n",
      "Train Epoch: 42 [12288/60000 (20%)]\tLoss: 107.671844\n",
      "Train Epoch: 42 [16384/60000 (27%)]\tLoss: 100.567352\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 98.242134\n",
      "Train Epoch: 42 [24576/60000 (41%)]\tLoss: 99.899750\n",
      "Train Epoch: 42 [28672/60000 (48%)]\tLoss: 104.417999\n",
      "Train Epoch: 42 [32768/60000 (55%)]\tLoss: 104.156670\n",
      "Train Epoch: 42 [36864/60000 (61%)]\tLoss: 101.770523\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 104.720642\n",
      "Train Epoch: 42 [45056/60000 (75%)]\tLoss: 108.111725\n",
      "Train Epoch: 42 [49152/60000 (82%)]\tLoss: 105.694160\n",
      "Train Epoch: 42 [53248/60000 (89%)]\tLoss: 99.256927\n",
      "Train Epoch: 42 [57344/60000 (96%)]\tLoss: 101.845299\n",
      "====> Epoch: 42 Average loss: 101.9256\n",
      "====> Test set loss: 102.1275\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 104.125137\n",
      "Train Epoch: 43 [4096/60000 (7%)]\tLoss: 104.945450\n",
      "Train Epoch: 43 [8192/60000 (14%)]\tLoss: 106.256744\n",
      "Train Epoch: 43 [12288/60000 (20%)]\tLoss: 104.694771\n",
      "Train Epoch: 43 [16384/60000 (27%)]\tLoss: 104.426483\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 98.564728\n",
      "Train Epoch: 43 [24576/60000 (41%)]\tLoss: 103.177711\n",
      "Train Epoch: 43 [28672/60000 (48%)]\tLoss: 106.129486\n",
      "Train Epoch: 43 [32768/60000 (55%)]\tLoss: 103.442062\n",
      "Train Epoch: 43 [36864/60000 (61%)]\tLoss: 102.930679\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 97.375237\n",
      "Train Epoch: 43 [45056/60000 (75%)]\tLoss: 101.452499\n",
      "Train Epoch: 43 [49152/60000 (82%)]\tLoss: 101.138847\n",
      "Train Epoch: 43 [53248/60000 (89%)]\tLoss: 105.812271\n",
      "Train Epoch: 43 [57344/60000 (96%)]\tLoss: 100.835587\n",
      "====> Epoch: 43 Average loss: 101.8577\n",
      "====> Test set loss: 102.1308\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 108.250725\n",
      "Train Epoch: 44 [4096/60000 (7%)]\tLoss: 102.224731\n",
      "Train Epoch: 44 [8192/60000 (14%)]\tLoss: 100.712090\n",
      "Train Epoch: 44 [12288/60000 (20%)]\tLoss: 101.635391\n",
      "Train Epoch: 44 [16384/60000 (27%)]\tLoss: 102.456909\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 96.356461\n",
      "Train Epoch: 44 [24576/60000 (41%)]\tLoss: 98.856277\n",
      "Train Epoch: 44 [28672/60000 (48%)]\tLoss: 103.659164\n",
      "Train Epoch: 44 [32768/60000 (55%)]\tLoss: 105.627136\n",
      "Train Epoch: 44 [36864/60000 (61%)]\tLoss: 105.078339\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 103.702530\n",
      "Train Epoch: 44 [45056/60000 (75%)]\tLoss: 96.335922\n",
      "Train Epoch: 44 [49152/60000 (82%)]\tLoss: 101.223480\n",
      "Train Epoch: 44 [53248/60000 (89%)]\tLoss: 102.733978\n",
      "Train Epoch: 44 [57344/60000 (96%)]\tLoss: 99.401802\n",
      "====> Epoch: 44 Average loss: 101.7912\n",
      "====> Test set loss: 102.1061\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 102.492844\n",
      "Train Epoch: 45 [4096/60000 (7%)]\tLoss: 102.475418\n",
      "Train Epoch: 45 [8192/60000 (14%)]\tLoss: 96.362091\n",
      "Train Epoch: 45 [12288/60000 (20%)]\tLoss: 106.594528\n",
      "Train Epoch: 45 [16384/60000 (27%)]\tLoss: 95.005859\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 106.443512\n",
      "Train Epoch: 45 [24576/60000 (41%)]\tLoss: 106.522850\n",
      "Train Epoch: 45 [28672/60000 (48%)]\tLoss: 104.414383\n",
      "Train Epoch: 45 [32768/60000 (55%)]\tLoss: 103.041145\n",
      "Train Epoch: 45 [36864/60000 (61%)]\tLoss: 106.181808\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 103.917236\n",
      "Train Epoch: 45 [45056/60000 (75%)]\tLoss: 102.007675\n",
      "Train Epoch: 45 [49152/60000 (82%)]\tLoss: 104.370514\n",
      "Train Epoch: 45 [53248/60000 (89%)]\tLoss: 100.201950\n",
      "Train Epoch: 45 [57344/60000 (96%)]\tLoss: 99.794617\n",
      "====> Epoch: 45 Average loss: 101.8037\n",
      "====> Test set loss: 102.0795\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 105.538025\n",
      "Train Epoch: 46 [4096/60000 (7%)]\tLoss: 103.429268\n",
      "Train Epoch: 46 [8192/60000 (14%)]\tLoss: 105.285049\n",
      "Train Epoch: 46 [12288/60000 (20%)]\tLoss: 101.430603\n",
      "Train Epoch: 46 [16384/60000 (27%)]\tLoss: 97.715813\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 108.729065\n",
      "Train Epoch: 46 [24576/60000 (41%)]\tLoss: 107.880005\n",
      "Train Epoch: 46 [28672/60000 (48%)]\tLoss: 96.526001\n",
      "Train Epoch: 46 [32768/60000 (55%)]\tLoss: 107.687607\n",
      "Train Epoch: 46 [36864/60000 (61%)]\tLoss: 94.706154\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 96.652328\n",
      "Train Epoch: 46 [45056/60000 (75%)]\tLoss: 99.996979\n",
      "Train Epoch: 46 [49152/60000 (82%)]\tLoss: 102.660027\n",
      "Train Epoch: 46 [53248/60000 (89%)]\tLoss: 101.251915\n",
      "Train Epoch: 46 [57344/60000 (96%)]\tLoss: 101.845459\n",
      "====> Epoch: 46 Average loss: 101.7027\n",
      "====> Test set loss: 102.1572\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 99.093651\n",
      "Train Epoch: 47 [4096/60000 (7%)]\tLoss: 107.048149\n",
      "Train Epoch: 47 [8192/60000 (14%)]\tLoss: 108.023697\n",
      "Train Epoch: 47 [12288/60000 (20%)]\tLoss: 100.202278\n",
      "Train Epoch: 47 [16384/60000 (27%)]\tLoss: 104.965164\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 99.967499\n",
      "Train Epoch: 47 [24576/60000 (41%)]\tLoss: 103.648018\n",
      "Train Epoch: 47 [28672/60000 (48%)]\tLoss: 98.099831\n",
      "Train Epoch: 47 [32768/60000 (55%)]\tLoss: 107.016525\n",
      "Train Epoch: 47 [36864/60000 (61%)]\tLoss: 102.479919\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 102.525925\n",
      "Train Epoch: 47 [45056/60000 (75%)]\tLoss: 106.678864\n",
      "Train Epoch: 47 [49152/60000 (82%)]\tLoss: 100.191086\n",
      "Train Epoch: 47 [53248/60000 (89%)]\tLoss: 103.621094\n",
      "Train Epoch: 47 [57344/60000 (96%)]\tLoss: 101.399887\n",
      "====> Epoch: 47 Average loss: 101.7112\n",
      "====> Test set loss: 101.8323\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 98.640388\n",
      "Train Epoch: 48 [4096/60000 (7%)]\tLoss: 103.161621\n",
      "Train Epoch: 48 [8192/60000 (14%)]\tLoss: 101.216805\n",
      "Train Epoch: 48 [12288/60000 (20%)]\tLoss: 102.130470\n",
      "Train Epoch: 48 [16384/60000 (27%)]\tLoss: 95.859962\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 99.957870\n",
      "Train Epoch: 48 [24576/60000 (41%)]\tLoss: 99.610817\n",
      "Train Epoch: 48 [28672/60000 (48%)]\tLoss: 97.332466\n",
      "Train Epoch: 48 [32768/60000 (55%)]\tLoss: 105.475235\n",
      "Train Epoch: 48 [36864/60000 (61%)]\tLoss: 100.373657\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 101.662315\n",
      "Train Epoch: 48 [45056/60000 (75%)]\tLoss: 104.881279\n",
      "Train Epoch: 48 [49152/60000 (82%)]\tLoss: 100.574463\n",
      "Train Epoch: 48 [53248/60000 (89%)]\tLoss: 99.142609\n",
      "Train Epoch: 48 [57344/60000 (96%)]\tLoss: 98.269333\n",
      "====> Epoch: 48 Average loss: 101.6379\n",
      "====> Test set loss: 101.8040\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 100.214554\n",
      "Train Epoch: 49 [4096/60000 (7%)]\tLoss: 103.060684\n",
      "Train Epoch: 49 [8192/60000 (14%)]\tLoss: 95.799393\n",
      "Train Epoch: 49 [12288/60000 (20%)]\tLoss: 103.589355\n",
      "Train Epoch: 49 [16384/60000 (27%)]\tLoss: 99.384743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 102.229904\n",
      "Train Epoch: 49 [24576/60000 (41%)]\tLoss: 101.665688\n",
      "Train Epoch: 49 [28672/60000 (48%)]\tLoss: 101.288887\n",
      "Train Epoch: 49 [32768/60000 (55%)]\tLoss: 106.678345\n",
      "Train Epoch: 49 [36864/60000 (61%)]\tLoss: 97.683434\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 102.711014\n",
      "Train Epoch: 49 [45056/60000 (75%)]\tLoss: 106.213905\n",
      "Train Epoch: 49 [49152/60000 (82%)]\tLoss: 99.669861\n",
      "Train Epoch: 49 [53248/60000 (89%)]\tLoss: 103.259018\n",
      "Train Epoch: 49 [57344/60000 (96%)]\tLoss: 106.787827\n",
      "====> Epoch: 49 Average loss: 101.6163\n",
      "====> Test set loss: 101.8406\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 99.573112\n",
      "Train Epoch: 50 [4096/60000 (7%)]\tLoss: 97.720924\n",
      "Train Epoch: 50 [8192/60000 (14%)]\tLoss: 101.502121\n",
      "Train Epoch: 50 [12288/60000 (20%)]\tLoss: 104.755035\n",
      "Train Epoch: 50 [16384/60000 (27%)]\tLoss: 100.669182\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 102.862839\n",
      "Train Epoch: 50 [24576/60000 (41%)]\tLoss: 101.821373\n",
      "Train Epoch: 50 [28672/60000 (48%)]\tLoss: 99.385384\n",
      "Train Epoch: 50 [32768/60000 (55%)]\tLoss: 100.753990\n",
      "Train Epoch: 50 [36864/60000 (61%)]\tLoss: 96.715652\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 103.674301\n",
      "Train Epoch: 50 [45056/60000 (75%)]\tLoss: 98.776222\n",
      "Train Epoch: 50 [49152/60000 (82%)]\tLoss: 95.531219\n",
      "Train Epoch: 50 [53248/60000 (89%)]\tLoss: 102.762115\n",
      "Train Epoch: 50 [57344/60000 (96%)]\tLoss: 97.095978\n",
      "====> Epoch: 50 Average loss: 101.5396\n",
      "====> Test set loss: 102.0479\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).to(device)\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
